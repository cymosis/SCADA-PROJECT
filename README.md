# Unified SCADA System (NT-SCADA)

![SCADA System](https://img.shields.io/badge/System-SCADA-blue)
![Status](https://img.shields.io/badge/Status-Active-success)
![Kubernetes](https://img.shields.io/badge/Kubernetes-K8s-326CE5?logo=kubernetes&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?logo=docker&logoColor=white)
![Python](https://img.shields.io/badge/Python-3.9+-3776AB?logo=python&logoColor=white)
![InfluxDB](https://img.shields.io/badge/InfluxDB-2.7-22ADF6?logo=influxdb&logoColor=white)
![Grafana](https://img.shields.io/badge/Grafana-10.2-F46800?logo=grafana&logoColor=white)
![Kafka](https://img.shields.io/badge/Kafka-7.7-231F20?logo=apachekafka&logoColor=white)
![Flink](https://img.shields.io/badge/Flink-1.18-E6526F?logo=apacheflink&logoColor=white)

**NT-SCADA** (New-Tech SCADA) is a complete, open-source Supervisory Control and Data Acquisition platform built with modern big data technologies. It is designed to ingest, store, analyze, and visualize large volumes of analog and digital inputs from industrial sensors, and control actuators based on real-time analysis.

---

## Team Members

1.  **Narayan Anshu**
2.  **Sheillah Khaluvitsi**
3.  **Cynthia Mutisya**

**GitHub Repo**: [https://github.com/cymosis/SCADA-PROJECT](https://github.com/cymosis/SCADA-PROJECT)



---

## Overview

### The aim of the project is to develop a supervisory control and data acquisition platform based on open-source technologies. We implement ingesting, storing, analyzing, and visualizing large volumes of analog and digital inputs collected from the SWaT dataset.
---

## System Architecture


### Component Details

1.  **Data Sources**:

    *   **SWaT Dataset**: The SWAT (Secure Water Treatment) dataset was used in the implementation of our system.The dataset was provided by iTrust Centre for Research in Cybersecurity which is part of the Singapore University of Technology and Design. The data was collected from 51 sensors/ actuators across the six water treatment stages. The types of sensors include pressure sensors (25),flow (12), level (8), motor state (6) and analytical (4). The data consisted of two components: the normal data which was mainly collected on 22nd June 2020 and attack data which was collected on 20th July 2019. Kindly note that this data was in excel format.
    *   There were six types of attacks that occurred on the 20th July 2019 and their timestamps were captured in the attack data. Because of privacy issues, we will not be able to share the data publicly as part of the agreement with iTrust for using their data.
    *   **Data Pre-processing**
      The data underwent pre-processing so as to prepare it for the next data processing stages. The timestamps were converted to an ISO 8601 format,labelled the attacks on the attack data i.e. placing 0 (normal) and 1 (attack) in the indicated times when there was an attack, standardized the column names from OPC UA format to human readable format (e.g. "LIT101.Pv"      → "LIT 101") and did mapping of the columns.
     **Data Ingestion and Messaging**: The pre-processed data was then fed to Kafka. Only one kafka broker, two producers and two consumers were implemented.There were three topics in kafka i.e. scada.normal, scada.attacks and scada.analytics_timer. The two producers i.e. Normal data producer and attack data producer read the respective excel files using pandas.The data was shuffled to simulate to simulate real world randomness. The data was sent in small batches or chunks of 1000 for the case of normal data and chunks of 100 for the case of attack data with a small time sleep delay between the messages to simulate a stream. The normal data producer streamed SCADA data from normal.xlsx file to the scada.normal kafka topic. The attack data producer on the other hand streamed data from the attacks.xlsx file to the scada.attacks kafka topic. The sacada. analytics_timer topic was used to trigger the flink analytics job.
There were two consumers, the normal data consumer and the attack data consumer. The normal data consumer consumed data from the scada.normal topic and fed it to the normal  backet in InfluxDB. The attack data consumer on the other hand consumed data from the scada.attacks topic and fed it to the attack data bucket present in InfluxDB.

**Time series database**
InfluxDB had four types of buckets i.e. normal data bucket, attack data bucket, anomaly data bucket and kafka analytics bucket. 
The normal data bucket stored all raw normal SCADA data consumed from scada.noermal topic.
The attack data bucket stored all the raw attack data consumed from scada.attack topic.
The anomaly data bucket stored all the results from the anomaly detection process
The kafka Analytics bucket stored realtime performance metrics for the kafka infrastructure generated by the Flink Job.




6.  **Data Visualization & Control**:
    *   **Grafana**:
        *   This is the primary user interface used to visualize, the current state, anomalies and flink jobs metrics.for visualizing all sensor and actuator data (tabular, plots, dashboards) as it connects easily to InfluxDB and Kafka. It will display the results of our real-time anomaly detection and fine-grained classification. It can also be used to send *control commands* back to actuators (via a custom plugin or integration with Kafka).

7.  **Actuators (Output Control)**:
    *   **Industrial Equipment (Simulated)**: In a real SCADA system, these are physical devices but, in our project, we will likely simulate their behavior. Typically, Grafana via user interaction will send a control command to a Kafka topic. A Flink job will consume this command from Kafka and will then interface with the physical actuator but in our case, the actuator service will simply log the command or update a simulated state.



-

---

## Features

### Real-time Data Processing
- **High-throughput Ingestion**: Kafka handles thousands of sensor readings per second
- **Stream Processing**: Real-time anomaly detection using Apache Flink
- **Binary Classification**: Detects normal vs. anomalous sensor behavior
- **Fine-grained Classification**: Classifies sensor states into 7 categories (CRITICALLY_LOW, LOW, BELOW_OPTIMAL, OPTIMAL, ABOVE_OPTIMAL, HIGH, CRITICALLY_HIGH)

### Machine Learning & Analytics
- **Automated Model Training**: Batch jobs train Random Forest and Gradient Boosting models
- **Feature Engineering**: Extracts time-series features using `sktime`
- **Daily Statistics**: Computes aggregates, averages, and trends


### Data Visualization
- **Interactive Dashboards**: Pre-configured Grafana dashboards for real-time monitoring
- **Historical Analysis**: Query and visualize trends over time
- **Anomaly Alerts**: Visual indicators for detected anomalies
- **Tabular Views**: Comprehensive sensor and actuator data tables

### Scalability & Reliability
- **Kubernetes Orchestration**: Auto-scaling and self-healing infrastructure
- **High Availability**: Redundant components and automatic failover
- **Distributed Processing**: Workload distributed across multiple nodes
- **Data Persistence**: Reliable storage with InfluxDB time-series database

---

## Work Plan & Phases


*   **Phase 1: Setup & Data Simulation (Sheillah, Cynthia & Narayan)**
    *   Set up a local Kubernetes cluster.
    *   Use Helm charts to deploy Kafka, InfluxDB, and Grafana into Kubernetes.
    *   Develop the **SWaT Kafka Producer** to stream the dataset into our sensor-inputs topic.

*   **Phase 2: Storage & Visualization (Sheillah, Cynthia & Narayan)**
    *   Configure InfluxDB to store the incoming data.
    *   Connect Grafana to InfluxDB and build the first dashboards (Analog Input Plots, Tabular Data).

*   **Phase 3: Batch Processing & Model Training (Sheillah, Cynthia & Narayan)**
    *   Develop the **Apache Flink** batch jobs.
    *   Use `sktime` and `scikit-learn` within these jobs to train our two models.
    *   Store the trained models in a model registry.

*   **Phase 4: Real-time Stream Processing (Sheillah, Cynthia & Narayan)**
    *   Develop the **Kafka Streams** applications for anomaly detection (Pipeline 1) and fine-grained classification (Pipeline 2). They will consume the live sensor inputs, use the models from Phase 3, and produce results to new topics.

*   **Phase 5: Actuator Control & Closing the Loop (Sheillah, Cynthia & Narayan)**
    *   Develop the **Control Logic** service that listens for anomalies and publishes commands.
    *   Develop a "mock actuator" consumer that subscribes to the control-commands topic and simply logs what command it *would have* executed. This proves the control loop works without needing a physical valve.

---

## Getting Started (Local Docker Version)

*Note: While the target architecture uses Kubernetes, this repository currently includes a Docker Compose setup for easy local development.*

### Prerequisites
*   **Docker Desktop** (Windows/Mac) or **Docker Engine** (Linux)
*   **Docker Compose** v2.0+
*   Minimum **8GB RAM** allocated to Docker

### Installation

1.  **Clone the Repository**
    ```bash
    git clone https://github.com/cymosis/SCADA-PROJECT.git
    cd SCADA-PROJECT
    ```

2.  **Start the System**
    ```bash
    docker-compose up -d
    ```
    *This will pull necessary images, build custom components, and start all services.*

3.  **Verify Deployment**
    ```bash
    docker-compose ps
    ```

---

## Service Dashboard

Access the various components of the system using the following credentials:

| Service | URL | Username | Password | Description |
| :--- | :--- | :--- | :--- | :--- |
| **Grafana** | [http://localhost:3000](http://localhost:3000) | `admin` | `admin` | Main visualization dashboard |
| **InfluxDB** | [http://localhost:8086](http://localhost:8086) | `admin` | `adminpass123` | Time-series database UI |
| **Kafka UI** | [http://localhost:8080](http://localhost:8080) | - | - | Kafka cluster management |
| **Flink Dashboard** | [http://localhost:8081](http://localhost:8081) | - | - | Stream processing jobs |

> **Note**: InfluxDB Organization is `nt-scada` and Bucket is `scada_data`.

---

## Project Structure

```text
SCADA-PROJECT/
├── docker-compose.yml          # Main orchestration file
├── nt-scada/                   # Core application code
│   ├── producers/              # Data generators (Sensors/Actuators)
│   ├── stream/                 # Stream processing logic (Anomaly Detection)
│   ├── storage/                # InfluxDB consumer
│   ├── batch/                  # ML models and analytics
│   ├── dashboards/             # Grafana provisioning
│   └── README.md               # Detailed component documentation
├── Swat Data/                  # Reference datasets
└── Documentation/              # Additional guides and docs
```

---

## Troubleshooting

### Common Issues

**1. Services keep restarting**
*   Check logs: `docker-compose logs <service-name>`
*   Ensure you have enough memory allocated to Docker (8GB+ recommended).

**2. No data in Grafana**
*   Verify producers are running: `docker-compose logs sensor-producer`
*   Check InfluxDB connection: `docker-compose logs influx-consumer`
*   Wait a few minutes for the initial data pipeline to flush.

### Useful Commands

*   **View Logs**: `docker-compose logs -f`
*   **Restart Service**: `docker-compose restart <service-name>`
*   **Stop System**: `docker-compose down`
*   **Clean Reset**: `docker-compose down -v` (Deletes all data)

---


Contributions are welcome! Here's how you can help:

1. **Fork the Repository**
   ```bash
   git fork https://github.com/cymosis/SCADA-PROJECT.git
   ```

2. **Create a Feature Branch**
   ```bash
   git checkout -b feature/your-feature-name
   ```

3. **Make Your Changes**
   - Follow the existing code style
   - Add tests if applicable
   - Update documentation as needed

4. **Commit Your Changes**
   ```bash
   git commit -m "Add feature: your feature description"
   ```

5. **Push to Your Fork**
   ```bash
   git push origin feature/your-feature-name
   ```

6. **Submit a Pull Request**
   - Provide a clear description of your changes
   - Reference any related issues
   - Ensure all tests pass

### Areas for Contribution
- **Bug Fixes**: Help identify and fix issues
- **New Features**: Add new functionality or components
- **Documentation**: Improve or expand documentation
- **Testing**: Add or improve test coverage
- **UI/UX**: Enhance Grafana dashboards and visualizations
- **Performance**: Optimize processing pipelines

---



---

<p align="center">
  We acknowledge using the SWAT data set in this project
</p>

# Unified SCADA System (NT-SCADA)

![SCADA System](https://img.shields.io/badge/System-SCADA-blue)
![Status](https://img.shields.io/badge/Status-Active-success)
![Kubernetes](https://img.shields.io/badge/Kubernetes-K8s-326CE5?logo=kubernetes&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?logo=docker&logoColor=white)
![Python](https://img.shields.io/badge/Python-3.9+-3776AB?logo=python&logoColor=white)
![InfluxDB](https://img.shields.io/badge/InfluxDB-2.7-22ADF6?logo=influxdb&logoColor=white)
![Grafana](https://img.shields.io/badge/Grafana-10.2-F46800?logo=grafana&logoColor=white)
![Kafka](https://img.shields.io/badge/Kafka-7.7-231F20?logo=apachekafka&logoColor=white)
![Flink](https://img.shields.io/badge/Flink-1.18-E6526F?logo=apacheflink&logoColor=white)

**NT-SCADA** (New-Tech SCADA) is a complete, open-source Supervisory Control and Data Acquisition platform built with modern big data technologies. It is designed to ingest, store, analyze, and visualize large volumes of analog and digital inputs from industrial sensors, and control actuators based on real-time analysis.

---

## Team Members

1.  **Narayan Anshu**
2.  **Sheillah Khaluvitsi**
3.  **Cynthia Mutisya**

**GitHub Repo**: [https://github.com/cymosis/SCADA-PROJECT](https://github.com/cymosis/SCADA-PROJECT)



---

## Overview

### The aim of the project is to develop a supervisory control and data acquisition platform based on open-source technologies. We implement ingesting, storing, analyzing, and visualizing large volumes of analog and digital inputs collected from the SWaT dataset.
---

## System Architecture


### Component Details

1.  **Data Sources**:

    *   **SWaT Dataset**: The SWAT (Secure Water Treatment) dataset was used in the implementation of our system.The dataset was provided by iTrust Centre for Research in Cybersecurity which is part of the Singapore University of Technology and Design. The data was collected from 51 sensors/ actuators across the six water treatment stages. The types of sensors include pressure sensors (25),flow (12), level (8), motor state (6) and analytical (4). The data consisted of two components: the normal data which was mainly collected on 22nd June 2020 and attack data which was collected on 20th July 2019. Kindly note that this data was in excel format.
    *   There were six types of attacks that occurred on the 20th July 2019 and their timestamps were captured in the attack data. Because of privacy issues, we will not be able to share the data publicly as part of the agreement with iTrust for using their data.
2.   **Data Pre-processing**
      The data underwent pre-processing so as to prepare it for the next data processing stages. The timestamps were converted to an ISO 8601 format,labelled the attacks on the attack data i.e. placing 0 (normal) and 1 (attack) in the indicated times when there was an attack, standardized the column names from OPC UA format to human readable format (e.g. "LIT101.Pv"      → "LIT 101") and did mapping of the columns.
3. **Data Ingestion and Messaging**: The pre-processed data was then fed to Kafka. Only one kafka broker, two producers and two consumers were implemented.There were three topics in kafka i.e. scada.normal, scada.attacks and scada.analytics_timer. The two producers i.e. Normal data producer and attack data producer read the respective excel files using pandas.The data was shuffled to simulate to simulate real world randomness. The data was sent in small batches or chunks of 1000 for the case of normal data and chunks of 100 for the case of attack data with a small time sleep delay between the messages to simulate a stream. The normal data producer streamed SCADA data from normal.xlsx file to the scada.normal kafka topic. The attack data producer on the other hand streamed data from the attacks.xlsx file to the scada.attacks kafka topic. The sacada. analytics_timer topic was used to trigger the flink analytics job.
There were two consumers, the normal data consumer and the attack data consumer. The normal data consumer consumed data from the scada.normal topic and fed it to the normal  backet in InfluxDB. The attack data consumer on the other hand consumed data from the scada.attacks topic and fed it to the attack data bucket present in InfluxDB.

4. **Time series database**
InfluxDB had four types of buckets i.e. normal data bucket, attack data bucket, anomaly data bucket and kafka analytics bucket. 
The normal data bucket stored all raw normal SCADA data consumed from scada.noermal topic.
The attack data bucket stored all the raw attack data consumed from scada.attack topic.
The anomaly data bucket stored all the results from the anomaly detection process
The kafka Analytics bucket stored realtime performance metrics for the kafka infrastructure generated by the Flink Job.

5. **Flink**
   Flink was used for realtime stream processing. We implemented one Flink Job Manager that acted as the coordinator, a Flink Task Manager that was responsible for the actual processing of the analytics. There were  four task slots present.There was a Flink job submitter that was responsible for the auto-deployment of the jobs and a real time analytics timer that was responsible for triggering the analytics. The entire process ran as follows: the timer was triggered every 30 seconds, Flink received the timer event then triggered the Kafka Analytics processor. The Kafka analytics processor then generated consumer metrics (lag, rate, status), producer metrics (throughput, errors), topic metrics (size, messages/sec) and system health score (0-100). All these metrics were then written to the kafka analytics bucket where it was available for visualization by Grafana.
   
6. **Anomaly Detection**
   We implemented a multistep approach for anomaly dtection and classification. The anomaly detection process was done in four layers, the first layer was the z-score, second layer was rule based method, the third layer was the binary and fine grained model and the fourth layer was the pattern matching layer.
   
**Layer 1 (Statistical Z-Score method):** Z-score measures how many standard deviations a data point is from the mean. It shows if a data point os normal or unusual based on the historical data. It is usually given by:
   Z = (X-μ)/σ
   Where: X is the current data point
          μ is the mean
          σ is the set standard deviation
   We had four thresholds for our Z-score as described below:
   |z| < 2.0  →  NORMAL     (within 95% of data)
   |z| > 2.0  →  MODERATE   (unusual, 95% confidence)
   |z| > 3.0  →  HIGH       (very unusual, 99.7% confidence)
   |z| > 4.0  →  CRITICAL   (extremely unusual, 99.99% confidence)
   The reason for using Z score is that most of the data is usually falls within within ±3σ. If the reading is any value outside this range, then the value is abnormal. One drawback of this method is that it assumes a normal distribution which is usually not the case in most datasets and also doe not have context i.e. it does not understand the physical meaning of the values.
   
**Layer 2 (Rule Based Detection):** In this method, safety rules and operational limits are applied. These are rules that under normal operation should never be violated. The types of rules used here include:
Physical Constraints: We set physical  constraints on the datasets e.g. for the case of a temperature sensor, the temperature should not be below -10 and not above 100.
Critical ranges (Safety Zones): We defined normal operation condition in the dataset e.g. for the case of a tank water level, 0 is the minimum value and 1000 is the maximum value,then 200 is critically low and 900 is critically high and so on.
Valid state sets: Here, we defined the valid states of the pump where 0 represents off, 1 represents starting, 2 represents on and 3 represents stop
The rules made were based on physical laws, equipment specifications, safety standards and engineering experience. The limitations of this method is however the following: requires complete set of rules so as to define all constraints, it is time consuming to build the rule set, may not adapt to system changes as needed and therefore might miss some attacks.

**Layer 3 (Machine Learning Detection):** In this method, we used trained machie learning models to learn patterns in the data. Two types of models were used,binary model and fine frained model.
Binary Model: The purpose of the Binary model was to classify the data point as either Normal(0) or attack(1). The Random Forest Classifier was used in this case. The classifier used 100 decision trees and each tree voted. The majority won. The model was trained on the entire data, both normal and attack data.
Fine-grained model: The purpose of this model was to classify the attacks into eight specific types. The Multi-class Random Forest Classifier was used in this case. The eight types of classes include NORMAL, DOS, NMRI, CMRI, SSCP, SSMP, MSCP, MSMP, RECON. The model was  trained on the labelled attack data.

Attack type mapping was done at this stage as below:
0: 'NORMAL',
    1: 'DOS',    # Denial of Service
    2: 'NMRI',   # Naive Malicious Response Injection
    3: 'CMRI',   # Complex Malicious Response Injection
    4: 'SSCP',   # Single Stage Single Point
    5: 'SSMP',   # Single Stage Multi Point
    6: 'MSCP',   # Multi Stage Single Point
    7: 'MSMP',   # Multi Stage Multi Point
    8: 'RECON'   # Reconnaissance
    
   The reason for using Random Forest was that it was able to handle the features well, it could handle non liear relationships and it worked well with imbalanced data. The limitations include, hard to explain why it classified somthing the way it did, it could memorize training data leading to overfitting and new attacks could be misclassified as it only classifies based on what it has seen.
   
**Layer 4 (Pattern Based Classification):** This section dealt with the analyzing of structure and scope of the anomalies so as to classify the attack type based on number of sensors affected, number of stages affected, type of sensors and historical patterns. We used the academic attack taxonomy from Goh et al.(2016).

DOS (denial of Service): ALL sensors showed constant values. There was no variation over time.
NMRI (Naive Malicious Response Injection): Characterised by 1-2 actuators in the wrong state.
CMRI (Complex Malicious Response Injection): Characterised by 3+ actuators having coordinated changes.
SSCP (Single Stage Single Point): Characterised by 1-2 sensors affected in one stage.
SSMP (Single Stage Multi Point):Characterised by 3+ sensors being affected in the same stage.Target one process stage.
MSCP (Multi Stage Single Point): Characterised by Same sensor type across 2+ stages being affected. Target a specific sensor family
MSMP (Multi Stage Multi Point): Characterised by 5+ sensors across 3+ stages being affected.
RECON (Reconnaissance): Characterized by Rapid sequential sensor access and Frequent small anomalies.
The advantages of this method is that it does not require any training and it is context aware. The limitations however are that it requires anomalies from Layers 1 - 3, has a fixed taxonomy and it cannot adapt to new patterns automatically.

Each anomaly event is then written to the Anomaly data bucket in influxDB. These results will then be visualized in Grafana.

7. **Visualization:** The system used Grafana to visualize both the industrial process and the anomaly metrics. Two dashboards were created, a SCADA operations monitoring dashboard and an anomaly detection dashboard.
   SCADA Operation Monitoring Dashboard: This was responsible for monitoring normal SCADA Operations. Data used was from the norml data bucket present in influxDB. It displayed the sensor values at the different water treatment stages including the status of the actuators.
   Anomaly Detection Dashboard: This was used to display the performance and results of the anomaly detection system. It consumed data from the anomaly data bucket present in influxDB. 11 panels were implemented in this regard.

┌─────────────────────────────────────────────────────────────────────────┐
│                      COMPLETE NT-SCADA DATA FLOW                        │
└─────────────────────────────────────────────────────────────────────────┘

[Excel Files] → [Preprocessing] → [Renamed Excel]
  22,628 rows       data_mapping      78 columns
  14,996 rows         .ipynb          standardized
      |                                    |
      ↓                                    ↓
      
      └────────────────┬───────────────────┘
                       ↓
              ┌────────┴────────┐
              │   2 PRODUCERS   │
              │  normal_data.py │ → scada.normal topic
              │  attack.py      │ → scada.attacks topic
              └────────┬────────┘
                       ↓
              ____________________
              │   KAFKA BROKER   │
              │   3 topics:      │
              │   - scada.normal │
              │   - scada.attacks│
              │   - analytics    │
              └────────┬────────┘
                       ↓
         ______________________
         |                    |
         ↓                    ↓              
   [2 CONSUMERS]          [FLINK]      
         ↓                   ↓
         
   [INFLUXDB]           [INFLUXDB]
   2 buckets               1 bucket
   - normal_data           - kafka_analytics
   - attack_data
         ↓
     
   [ANOMALY DETECTOR]
   (4-layer hybrid)
         ↓
     
   [INFLUXDB]
   - anomaly_data bucket
         ↓
     
   [GRAFANA]
   - SCADA dashboard
   - Anomaly dashboard
     
         ↓
   [USER MONITORING]


## Work Plan & Phases


*   **Phase 1: Setup & Data Simulation (Sheillah, Cynthia & Narayan)**
    *   Set up a local Kubernetes cluster.
    *   Use Helm charts to deploy Kafka, InfluxDB, and Grafana into Kubernetes.
    *   Develop the **SWaT Kafka Producer** to stream the dataset into our sensor-inputs topic.

*   **Phase 2: Storage & Visualization (Sheillah, Cynthia & Narayan)**
    *   Configure InfluxDB to store the incoming data.
    *   Connect Grafana to InfluxDB and build the first dashboards (Analog Input Plots, Tabular Data).

*   **Phase 3: Batch Processing & Model Training (Sheillah, Cynthia & Narayan)**
    *   Develop the **Apache Flink** batch jobs.
    *   Use `sktime` and `scikit-learn` within these jobs to train our two models.
    *   Store the trained models in a model registry.

*   **Phase 4: Real-time Stream Processing (Sheillah, Cynthia & Narayan)**
    *   Develop the **Kafka Streams** applications for anomaly detection (Pipeline 1) and fine-grained classification (Pipeline 2). They will consume the live sensor inputs, use the models from Phase 3, and produce results to new topics.

*   **Phase 5: Actuator Control & Closing the Loop (Sheillah, Cynthia & Narayan)**
    *   Develop the **Control Logic** service that listens for anomalies and publishes commands.
    *   Develop a "mock actuator" consumer that subscribes to the control-commands topic and simply logs what command it *would have* executed. This proves the control loop works without needing a physical valve.

---

## Getting Started (Local Docker Version)

*Note: While the target architecture uses Kubernetes, this repository currently includes a Docker Compose setup for easy local development.*

### Prerequisites
*   **Docker Desktop** (Windows/Mac) or **Docker Engine** (Linux)
*   **Docker Compose** v2.0+
*   Minimum **8GB RAM** allocated to Docker

### Installation

1.  **Clone the Repository**
    ```bash
    git clone https://github.com/cymosis/SCADA-PROJECT.git
    cd SCADA-PROJECT
    ```

2.  **Start the System**
    ```bash
    docker-compose up -d
    ```
    *This will pull necessary images, build custom components, and start all services.*

3.  **Verify Deployment**
    ```bash
    docker-compose ps
    ```

---

## Service Dashboard

Access the various components of the system using the following credentials:

| Service | URL | Username | Password | Description |
| :--- | :--- | :--- | :--- | :--- |
| **Grafana** | [http://localhost:3000](http://localhost:3000) | `admin` | `admin` | Main visualization dashboard |
| **InfluxDB** | [http://localhost:8086](http://localhost:8086) | `admin` | `adminpass123` | Time-series database UI |
| **Kafka UI** | [http://localhost:8080](http://localhost:8080) | - | - | Kafka cluster management |
| **Flink Dashboard** | [http://localhost:8081](http://localhost:8081) | - | - | Stream processing jobs |

> **Note**: InfluxDB Organization is `nt-scada` and Bucket is `scada_data`.

---

## Project Structure

```text
SCADA-PROJECT/
├── docker-compose.yml          # Main orchestration file
├── nt-scada/                   # Core application code
│   ├── producers/              # Data generators (Sensors/Actuators)
│   ├── stream/                 # Stream processing logic (Anomaly Detection)
│   ├── storage/                # InfluxDB consumer
│   ├── batch/                  # ML models and analytics
│   ├── dashboards/             # Grafana provisioning
│   └── README.md               # Detailed component documentation
├── Swat Data/                  # Reference datasets
└── Documentation/              # Additional guides and docs
```

---

## Troubleshooting

### Common Issues

**1. Services keep restarting**
*   Check logs: `docker-compose logs <service-name>`
*   Ensure you have enough memory allocated to Docker (8GB+ recommended).

**2. No data in Grafana**
*   Verify producers are running: `docker-compose logs sensor-producer`
*   Check InfluxDB connection: `docker-compose logs influx-consumer`
*   Wait a few minutes for the initial data pipeline to flush.

### Useful Commands

*   **View Logs**: `docker-compose logs -f`
*   **Restart Service**: `docker-compose restart <service-name>`
*   **Stop System**: `docker-compose down`
*   **Clean Reset**: `docker-compose down -v` (Deletes all data)

---


Contributions are welcome! Here's how you can help:

1. **Fork the Repository**
   ```bash
   git fork https://github.com/cymosis/SCADA-PROJECT.git
   ```

2. **Create a Feature Branch**
   ```bash
   git checkout -b feature/your-feature-name
   ```

3. **Make Your Changes**
   - Follow the existing code style
   - Add tests if applicable
   - Update documentation as needed

4. **Commit Your Changes**
   ```bash
   git commit -m "Add feature: your feature description"
   ```

5. **Push to Your Fork**
   ```bash
   git push origin feature/your-feature-name
   ```

6. **Submit a Pull Request**
   - Provide a clear description of your changes
   - Reference any related issues
   - Ensure all tests pass

### Areas for Contribution
- **Bug Fixes**: Help identify and fix issues
- **New Features**: Add new functionality or components
- **Documentation**: Improve or expand documentation
- **Testing**: Add or improve test coverage
- **UI/UX**: Enhance Grafana dashboards and visualizations
- **Performance**: Optimize processing pipelines

---



---

<p align="center">
  We acknowledge using the SWAT data set in this project
</p>
